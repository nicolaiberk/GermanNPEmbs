{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "emb_model.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5825374-c511-47fe-a2e4-da9bd8431a6d",
        "outputId": "d6210043-89e0-4704-ac1a-3f73a1e707c6"
      },
      "source": [
        "## estimate word embeddings from newspaper data\n",
        "## code adapted from https://github.com/damian0604/embeddingworkshop/blob/main/04exercise.ipynb\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "\n",
        "# tqdm allows you to display progress bars in loops\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import gensim\n",
        "\n",
        "# lets get more output\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "id": "c5825374-c511-47fe-a2e4-da9bd8431a6d",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11c85d9f-4750-470f-8ff8-a0a91dc705c6",
        "outputId": "70cbff16-0c17-4b97-cab2-cba409b85b1f"
      },
      "source": [
        "# get full set of news articles\n",
        "!rm sample_data -r\n",
        "!mkdir newspapers\n",
        "!wget -O newspapers/articles.zip https://www.dropbox.com/sh/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa?dl=0\n",
        "!unzip newspapers/articles.zip -d newspapers\n",
        "!rm newspapers/articles.zip"
      ],
      "id": "11c85d9f-4750-470f-8ff8-a0a91dc705c6",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'sample_data': No such file or directory\n",
            "mkdir: cannot create directory ‘newspapers’: File exists\n",
            "--2021-07-21 12:55:36--  https://www.dropbox.com/sh/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /sh/raw/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa [following]\n",
            "--2021-07-21 12:55:37--  https://www.dropbox.com/sh/raw/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucac7fba53c8f7d43bcdc7d75101.dl.dropboxusercontent.com/zip_download_get/A2HJQS3MTgvZooc9K2B4WR5YpH7cOfmhThPyuPKRdIdSjkxNYs-aL6NwtuoM2ZWqaGAQIEk6Is5hcaEnB1AmfGfp_grsUqFD7qIApyK0x2L00Q# [following]\n",
            "--2021-07-21 12:55:38--  https://ucac7fba53c8f7d43bcdc7d75101.dl.dropboxusercontent.com/zip_download_get/A2HJQS3MTgvZooc9K2B4WR5YpH7cOfmhThPyuPKRdIdSjkxNYs-aL6NwtuoM2ZWqaGAQIEk6Is5hcaEnB1AmfGfp_grsUqFD7qIApyK0x2L00Q\n",
            "Resolving ucac7fba53c8f7d43bcdc7d75101.dl.dropboxusercontent.com (ucac7fba53c8f7d43bcdc7d75101.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to ucac7fba53c8f7d43bcdc7d75101.dl.dropboxusercontent.com (ucac7fba53c8f7d43bcdc7d75101.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6202295648 (5.8G) [application/zip]\n",
            "Saving to: ‘newspapers/articles.zip’\n",
            "\n",
            "newspapers/articles 100%[===================>]   5.78G  19.3MB/s    in 5m 54s  \n",
            "\n",
            "2021-07-21 13:01:32 (16.7 MB/s) - ‘newspapers/articles.zip’ saved [6202295648/6202295648]\n",
            "\n",
            "Archive:  newspapers/articles.zip\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            "replace newspapers/_sz_articles.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            " extracting: newspapers/_sz_articles.csv  \n",
            " extracting: newspapers/_taz_articles.csv  \n",
            " extracting: newspapers/_faz_articles.csv  \n",
            " extracting: newspapers/_spon_articles.csv  \n",
            " extracting: newspapers/_bild_articles.csv  \n",
            " extracting: newspapers/_sz_articles_2019.csv  \n",
            " extracting: newspapers/_taz_articles_2019.csv  \n",
            " extracting: newspapers/_faz_articles_2019.csv  \n",
            " extracting: newspapers/_bild_articles_2019.csv  \n",
            " extracting: newspapers/_spon_articles_2019.csv  \n",
            " extracting: newspapers/_weltonline_articles.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz3OYCGmcSXU",
        "outputId": "30b939bc-7e91-49ac-a634-c1cd04590c7e"
      },
      "source": [
        "# load all texts\n",
        "if 'artcls' in locals():\n",
        "  del(artcls)\n",
        "\n",
        "for filename in tqdm(os.listdir('newspapers')):\n",
        "  if 'artcls' in locals():\n",
        "    print(f'\\nLoaded {artcls.shape[0]} articles')\n",
        "    artcls = artcls.append(pd.read_csv('newspapers/'+filename))\n",
        "  else:\n",
        "    artcls = pd.read_csv('newspapers/'+filename)\n",
        "print(f'Loaded {artcls.shape[0]} articles, done.')"
      ],
      "id": "lz3OYCGmcSXU",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  9%|▉         | 1/11 [00:15<02:35, 15.55s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 263266 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 2/11 [00:29<02:15, 15.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 575291 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 3/11 [00:45<02:02, 15.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 794334 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 36%|███▋      | 4/11 [00:49<01:22, 11.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 893180 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 45%|████▌     | 5/11 [01:07<01:22, 13.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1220020 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 55%|█████▍    | 6/11 [01:11<00:54, 10.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1285387 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▎   | 7/11 [01:20<00:40, 10.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1436035 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 8/11 [01:23<00:24,  8.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1472453 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 9/11 [01:28<00:14,  7.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1545861 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 91%|█████████ | 10/11 [01:30<00:05,  5.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1634513 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [02:00<00:00, 10.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 2474182 articles, done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "wPBsxGmEgp1W",
        "outputId": "794bad6a-4f61-41a2-9e75-d759f1607e5e"
      },
      "source": [
        "artcls = artcls.reset_index()\n",
        "artcls.text[0]"
      ],
      "id": "wPBsxGmEgp1W",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'was fehlt ... ... der Vorsatz    Der Vorsatz hat keinen guten Klang: Mit Vorsatz gehandelt zu haben, wird einem meist im Gericht vorgeworfen. Auch die guten Vorsätze haben immer den unangenehmen Beigeschmack von Schuld und Sühne - nicht zuletzt, weil sie zu 90 Prozent gebrochen werden. Woher der Brauch kommt, sich im neuen Jahr eine Änderung des Verhaltens vorzunehmen, ist unklar.    Am wahrscheinlichsten ist ein christlicher Ursprung, wie bei vielen Festtagsbräuchen - immerhin stammt das Wort Silvester vom Namenstag des Papstes Silvester (lateinisch für \"Waldmensch\"), der am 31. Dezember 335 starb. Möglicherweise sind die guten Vorsätze also eine katholische Erfindung: Die Sünden werden vergeben, aber nur, wenn man Besserung gelobt.    Die Wortherkunft der guten Vorsätze ist leichter zu bestimmen: Die Wurzel des Guten liegt im germanischen \"goda\" (passend, geeignet), das sich im 8. Jahrhundert zu \"guot\" (Besitz, Vermögen) weiterentwickelte. Vorsätze hießen im Mittelhochdeutschen \"vürsaz\" (Vorhaben, Absicht) und sind vom althochdeutschen \"sezzen\" (aufstellen, festlegen) abgeleitet, welches wiederum vom germanischen \"set-ja\" (sitzen) abstammt.    Was man gut findet, also was einem gerade \"passt\", kann morgen schon wieder stören, wie es mit den guten Vorsätzen meist ist. Schöner als das säuerlich-christliche Bekenntnis zur Besserung wäre es ohnehin, die Tabula rasa des neuen Jahres zu nutzen, um den in einigen Kulturen verbreiteten Brauch zu praktizieren, allen Streit und Ärger des vergangenen Jahres zu vergessen, alle Schuld zu erlassen, alle Fehler zu vergeben - ohne Gegenleistung. Wäre das nicht mal ein guter Vorsatz? (WENK)    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myJ1k4uEvZSb",
        "outputId": "66378b8d-308b-442f-efa8-7b9cc7da0850"
      },
      "source": [
        "artcls.shape"
      ],
      "id": "myJ1k4uEvZSb",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2474182, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "p4w1ixMvg7Zl",
        "outputId": "f8076977-7a3a-4606-807b-665730bea8d9"
      },
      "source": [
        "# check if string\n",
        "stringvar = [str == type(i) for i in artcls.text]\n",
        "artcls = artcls[stringvar]\n",
        "\n",
        "# cut into sentences\n",
        "trans = str.maketrans('', '', string.punctuation) # translation scheme for removing punctuation\n",
        "uniquesentences = set()\n",
        "for review in tqdm(artcls.text):\n",
        "    for sentence in sent_tokenize(review):\n",
        "        # remove HTML tags in there\n",
        "        sentence = re.sub(r\"<.*?>\",\" \",sentence)\n",
        "        sentence = sentence.translate(trans) \n",
        "        if sentence not in uniquesentences:\n",
        "            uniquesentences.add(sentence.lower())\n",
        "\n",
        "print(f\"We now have {len(uniquesentences)} unique sentences.\")"
      ],
      "id": "p4w1ixMvg7Zl",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-50167cd9ae89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# check if string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstringvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0martcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0martcls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstringvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# cut into sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'artcls' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrSZs-UZ5UoU"
      },
      "source": [
        "del(artcls)"
      ],
      "id": "OrSZs-UZ5UoU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIRoSblgFC8H"
      },
      "source": [
        "with open('drive/MyDrive/uniquesentences.txt', mode='w') as fo:\n",
        "  for sentence in uniquesentences:\n",
        "    fo.write(sentence)\n",
        "\n",
        "del(uniquesentences)"
      ],
      "id": "RIRoSblgFC8H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k91FQsjGsLxd"
      },
      "source": [
        "tokenizedsentences = []\n",
        "\n",
        "with open('drive/MyDrive/uniquesentences.txt', mode='r') as fi:\n",
        "  reader = csv.reader(fi)\n",
        "  for sentence in reader:\n",
        "    tokenizedsentences.append(sentence.split())"
      ],
      "id": "k91FQsjGsLxd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spkoV479rBV9"
      },
      "source": [
        "print(f\"Started setting up the model at {datetime.now()}\")\n",
        "model = gensim.models.Word2Vec(size=300, min_count=100) # we want 300 dimensions and not overdo it with the features\n",
        "model.build_vocab(tokenizedsentences)\n",
        "print(f\"Started training at {datetime.now()}\")\n",
        "model.train(tokenizedsentences, total_examples=model.corpus_count,  epochs=5)\n",
        "# our model gets better if we use more epochs, but we can only do so if we use a list instead of a generator as input\n",
        "# after all, you can only pass over a generator once.\n",
        "# model.train(tokenizedsentences2, total_examples=model.corpus_count,  epochs=model.epochs)\n",
        "print(f\"Finished training at {datetime.now()}\")"
      ],
      "id": "spkoV479rBV9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-2XB8dQu6Cl"
      },
      "source": [
        "model.save(\"np_emb\")"
      ],
      "id": "J-2XB8dQu6Cl",
      "execution_count": null,
      "outputs": []
    }
  ]
}