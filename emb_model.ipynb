{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Kopie von emb_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/GermanNPEmbs/blob/main/emb_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5825374-c511-47fe-a2e4-da9bd8431a6d",
        "outputId": "85b612ad-4598-47f4-d688-d4b8b70ea8bf"
      },
      "source": [
        "## estimate word embeddings from newspaper data\n",
        "## code adapted from https://github.com/damian0604/embeddingworkshop/blob/main/04exercise.ipynb\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "import sys\n",
        "import ast\n",
        "import time\n",
        "\n",
        "\n",
        "# tqdm allows you to display progress bars in loops\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import gensim\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "# lets get more output\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "id": "c5825374-c511-47fe-a2e4-da9bd8431a6d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11c85d9f-4750-470f-8ff8-a0a91dc705c6"
      },
      "source": [
        "# get full set of news articles\n",
        "if not os.path.isfile('newspapers/_bild_articles.csv'):\n",
        "    os.system('mkdir newspapers')\n",
        "    os.system('wget -O newspapers/articles.zip https://www.dropbox.com/sh/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa?dl=0')\n",
        "    os.system('unzip newspapers/articles.zip -d newspapers')\n",
        "    os.system('rm newspapers/articles.zip')"
      ],
      "id": "11c85d9f-4750-470f-8ff8-a0a91dc705c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz3OYCGmcSXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e446cb2-7b41-4a4f-d59c-7fe7456dc46c"
      },
      "source": [
        "# load all texts\n",
        "if 'artcls' not in locals():\n",
        "  for filename in tqdm(os.listdir('newspapers')):\n",
        "    if 'artcls' in locals():\n",
        "      print(f'\\nLoaded {artcls.shape[0]} articles')\n",
        "      artcls = artcls.append(pd.read_csv('newspapers/'+filename))\n",
        "    else:\n",
        "      artcls = pd.read_csv('newspapers/'+filename)\n",
        "  print(f'Loaded {artcls.shape[0]} articles, done.')\n",
        "\n",
        "  artcls = artcls.reset_index()\n",
        "\n",
        "\n",
        "# keep only if string\n",
        "stringvar = [str == type(i) for i in artcls.text]\n",
        "artcls = artcls[stringvar]\n",
        "del(stringvar)\n",
        "\n",
        "print(artcls.text[0])"
      ],
      "id": "lz3OYCGmcSXU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  9%|▉         | 1/11 [00:04<00:44,  4.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 150648 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 2/11 [00:06<00:33,  3.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 249494 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 3/11 [00:07<00:22,  2.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 338146 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 36%|███▋      | 4/11 [00:10<00:20,  2.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 411554 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 45%|████▌     | 5/11 [00:23<00:35,  5.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 630597 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 55%|█████▍    | 6/11 [00:46<00:54, 10.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 942622 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▎   | 7/11 [01:01<00:48, 12.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1269462 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 8/11 [01:10<00:33, 11.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1532728 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 9/11 [01:13<00:17,  9.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1598095 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 91%|█████████ | 10/11 [01:16<00:07,  7.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1634513 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [01:42<00:00,  9.33s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 2474182 articles, done.\n",
            "Zum ersten Mal seit dem Amoklauf von Newtown sind in den USA Befürworter und Gegner von schärferen Waffengesetzen vor den Senat getreten. Die frühere demokratische Abgeordnete Gabrielle Giffords, selbst Opfer einer Schusswaffen-Attacke, sagte an ihre ehemaligen Kollegen gerichtet: \"Zu viele Kinder sterben. Zu viele Kinder. Wir müssen etwas unternehmen!\" Giffords rief den Kongress zum Handeln auf. \"Wir müssen etwas tun. Es wird schwer sein, aber jetzt ist die Zeit.\" Giffords war im Januar 2011 bei einem Besuch in ihrem Wahlkreis im Bundesstaat Arizona von einem jungen Mann aus nächster Nähe in den Kopf geschossen worden. Die Politikerin überlebte schwer verletzt. Bei der Attacke starben sechs Menschen, unter ihnen ein neunjähriges Mädchen. Giffords wurde von ihrem Ehemann Mark Kelly begleitet. Die Politikerin und der Ex-Astronaut hatten Anfang Januar die Initiative \"Americans for Responsible Solutions\" (Amerikaner für verantwortungsbewusste Lösungen) gegründet. Wayne LaPierre von der Waffenlobbyorganisation NRA sagte, man müsse bestehende Gesetze zur Überprüfung von Waffenkäufern anwenden, anstatt neue Gesetze einzuführen. Gesetzestreue Waffenbesitzer würden nicht die Schuld für gewalttätige oder geistesgestörte Kriminelle auf sich nehmen, betonte LaPierre. \"Wir glauben auch nicht, dass die Regierung vorschreiben sollte, welche Waffen wir gesetzmäßig besitzen und nutzen dürfen, um unsere Familien zu schützen\", sagte er weiter. \"Eine Kugel in der Hand eines Geisteskranken ist zu viel - sechs Kugeln in der Hand einer Mutter, die ihre neun Jahre alten Kinder schützen möchte, sind möglicherweise zu wenig\", argumentierte der Republikaner Chuck Grassley. Die mit etwa zwei Dutzend Teilnehmern besetzte Runde diskutierte auch die von Präsident Barack Obama vorgestellten Maßnahmen gegen Waffengewalt. Während eine verbindliche Überprüfung jedes Waffenkäufers auf psychische Vorerkrankungen eher Zustimmung erzielte, lehnten einige konservative Vertreter eine maximal erlaubte Patronenzahl für Waffenmagazine oder gar ein generelles Verbot von halbautomatischen Sturmgewehren ab. In den USA hatten das Kino-Attentat in Aurora mit zwölf Toten und die Attacke auf die Sandy Hook Grundschule in Newtown eine hitzige Debatte über schärfere Waffenrechte ausgelöst. Die Verfassung garantiert das Recht auf Waffenbesitz, rund 300 Millionen Waffen befinden sich laut Schätzungen in den USA in Privatbesitz. Rund 11.000 Menschen werden in den Vereinigten Staaten pro Jahr Opfer von Verbrechen mit Waffengewalt.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN7u7PUswUWQ"
      },
      "source": [
        "# subset\n",
        "artcls = artcls.text"
      ],
      "id": "SN7u7PUswUWQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpA0PEMXdFAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ee5062-10d0-499f-c70a-bb4e25027348"
      },
      "source": [
        "# cut into sentences\n",
        "print('\\nCutting into sentences:')\n",
        "uniquesentences = set()\n",
        "trans = str.maketrans('', '', string.punctuation) # translation scheme for removing punctuation\n",
        "for review in tqdm(artcls):\n",
        "  sentences = sent_tokenize(review) \n",
        "  for sentence in sentences:\n",
        "    sent_trans = sentence.translate(trans).lower()\n",
        "    if sent_trans not in uniquesentences:\n",
        "      uniquesentences.add(sent_trans)\n",
        "\n",
        "del(artcls)"
      ],
      "id": "zpA0PEMXdFAF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 93/2214853 [00:00<39:50, 926.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cutting into sentences:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2214853/2214853 [42:14<00:00, 873.72it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCRZzFiXkw9t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4ea2e7d-0b50-4d67-dd54-c51383321a25"
      },
      "source": [
        "# extract \n",
        "print(f\"\\nWe now have {len(uniquesentences)} unique sentences.\")"
      ],
      "id": "oCRZzFiXkw9t",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "We now have 42301471 unique sentences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNK2emCxgmeT"
      },
      "source": [
        "# with open('uniquesentences.txt', 'w') as fo:\n",
        "#   writer = csv.writer(fo)\n",
        "#   for sentence in tqdm(uniquesentences):\n",
        "#     writer.writerow([sentence])"
      ],
      "id": "WNK2emCxgmeT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUlu9tof34Zs"
      },
      "source": [
        "# del(uniquesentences)"
      ],
      "id": "tUlu9tof34Zs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k91FQsjGsLxd"
      },
      "source": [
        "# print('Append split sentences')\n",
        "# tokenizedsentences = []\n",
        "# i = 0\n",
        "# with open('uniquesentences.txt', mode = 'r') as fi:\n",
        "#     reader = csv.reader(fi)\n",
        "#     next(reader)\n",
        "#     for sentence in tqdm(reader):\n",
        "#       tokenizedsentences.append(sentence[0].split())\n",
        "#       if i < 2:\n",
        "#         i += 1\n",
        "#         print(sentence[0].split())"
      ],
      "id": "k91FQsjGsLxd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0N3yLWzParU"
      },
      "source": [
        "tokenizedsentences = (sentence.split() for sentence in uniquesentences)\n",
        "tokenizedsentences2 = (sentence.split() for sentence in uniquesentences)\n",
        "del(uniquesentences)"
      ],
      "id": "U0N3yLWzParU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spkoV479rBV9"
      },
      "source": [
        "print(f\"Started setting up the model at {datetime.now()}\")\n",
        "model = gensim.models.Word2Vec(size=300, min_count=100) # we want 300 dimensions and not overdo it with the features\n",
        "model.build_vocab(tokenizedsentences)\n",
        "print(f\"Started training at {datetime.now()}\")\n",
        "model.train(tokenizedsentences2, total_examples=model.corpus_count,  epochs=1)\n",
        "print(f\"Finished training at {datetime.now()}\")"
      ],
      "id": "spkoV479rBV9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-2XB8dQu6Cl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffb0985b-5499-4a68-8ab1-47a0ca91fdfc"
      },
      "source": [
        "print('Saving model:')\n",
        "model.save(\"np_emb\")\n",
        "print('Model finished!')"
      ],
      "id": "J-2XB8dQu6Cl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-31 12:46:21,822 : INFO : saving Word2Vec object under np_emb, separately None\n",
            "2021-07-31 12:46:21,828 : INFO : storing np array 'vectors' to np_emb.wv.vectors.npy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving model:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-31 12:46:22,022 : INFO : not storing attribute vectors_norm\n",
            "2021-07-31 12:46:22,025 : INFO : storing np array 'syn1neg' to np_emb.trainables.syn1neg.npy\n",
            "2021-07-31 12:46:22,387 : INFO : not storing attribute cum_table\n",
            "2021-07-31 12:46:22,912 : INFO : saved np_emb\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ha2RB9E8uED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f559c1-564c-4d55-e03e-8c9d6c20625a"
      },
      "source": [
        "# Store just the words + their trained embeddings.\n",
        "word_vectors = model.wv\n",
        "word_vectors.save(\"word2vec.wordvectors\")"
      ],
      "id": "9ha2RB9E8uED",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-31 12:46:22,924 : INFO : saving Word2VecKeyedVectors object under word2vec.wordvectors, separately None\n",
            "2021-07-31 12:46:22,926 : INFO : storing np array 'vectors' to word2vec.wordvectors.vectors.npy\n",
            "2021-07-31 12:46:23,227 : INFO : not storing attribute vectors_norm\n",
            "2021-07-31 12:46:23,650 : INFO : saved word2vec.wordvectors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXCjolwu9MM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a80d38-eae0-4dfe-ae91-d99180e8fceb"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')"
      ],
      "id": "GXCjolwu9MM6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-31 12:47:15,971 : INFO : loading Word2VecKeyedVectors object from word2vec.wordvectors\n",
            "2021-07-31 12:47:16,505 : INFO : loading vectors from word2vec.wordvectors.vectors.npy with mmap=r\n",
            "2021-07-31 12:47:16,528 : INFO : setting ignored attribute vectors_norm to None\n",
            "2021-07-31 12:47:16,529 : INFO : loaded word2vec.wordvectors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjr7esAa9Py7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb07bbfd-5fe1-4b33-ad99-fd52924c42c5"
      },
      "source": [
        "word_vectors.most_similar('flüchtling', topn=10)  # get other similar words"
      ],
      "id": "wjr7esAa9Py7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-31 12:47:19,961 : INFO : precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('kriegsflüchtling', 0.7087794542312622),\n",
              " ('asylsuchender', 0.7084465026855469),\n",
              " ('migrant', 0.7026200890541077),\n",
              " ('syrer', 0.6834374666213989),\n",
              " ('asylbewerber', 0.674170732498169),\n",
              " ('häftling', 0.6647118926048279),\n",
              " ('afghane', 0.6590137481689453),\n",
              " ('kurde', 0.6281337738037109),\n",
              " ('muslim', 0.6219367980957031),\n",
              " ('terrorist', 0.6130465269088745)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}