{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5825374-c511-47fe-a2e4-da9bd8431a6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5825374-c511-47fe-a2e4-da9bd8431a6d",
    "outputId": "d6210043-89e0-4704-ac1a-3f73a1e707c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nico/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/nico/textanalysis/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## estimate word embeddings from newspaper data\n",
    "## code adapted from https://github.com/damian0604/embeddingworkshop/blob/main/04exercise.ipynb\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "# tqdm allows you to display progress bars in loops\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import gensim\n",
    "\n",
    "# lets get more output\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11c85d9f-4750-470f-8ff8-a0a91dc705c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11c85d9f-4750-470f-8ff8-a0a91dc705c6",
    "outputId": "70cbff16-0c17-4b97-cab2-cba409b85b1f"
   },
   "outputs": [],
   "source": [
    "# get full set of news articles\n",
    "if not os.path.isfile('newspapers/_bild_articles.csv') and not os.path.isfile('uniquesentences.txt'):\n",
    "    os.system('mkdir newspapers')\n",
    "    os.system('wget -O newspapers/articles.zip https://www.dropbox.com/sh/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa?dl=0')\n",
    "    os.system('unzip newspapers/articles.zip -d newspapers')\n",
    "    os.system('rm newspapers/articles.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lz3OYCGmcSXU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lz3OYCGmcSXU",
    "outputId": "30b939bc-7e91-49ac-a634-c1cd04590c7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('uniquesentences.txt'):\n",
    "    # load all texts\n",
    "    for filename in tqdm(os.listdir('newspapers')):\n",
    "      if 'artcls' in locals():\n",
    "        print(f'\\nLoaded {artcls.shape[0]} articles')\n",
    "        artcls = artcls.append(pd.read_csv('newspapers/'+filename))\n",
    "      else:\n",
    "        artcls = pd.read_csv('newspapers/'+filename)\n",
    "    print(f'Loaded {artcls.shape[0]} articles, done.')\n",
    "\n",
    "    artcls = artcls.reset_index()\n",
    "\n",
    "\n",
    "    # keep only if string\n",
    "    stringvar = [str == type(i) for i in artcls.text]\n",
    "    artcls = artcls[stringvar]\n",
    "\n",
    "    # cut into sentences\n",
    "    print('Cutting into sentences:')\n",
    "    trans = str.maketrans('', '', string.punctuation) # translation scheme for removing punctuation\n",
    "    uniquesentences = set()\n",
    "    for review in tqdm(artcls.text):\n",
    "        for sentence in sent_tokenize(review):\n",
    "            # remove HTML tags in there\n",
    "            sentence = re.sub(r\"<.*?>\",\" \",sentence)\n",
    "            sentence = sentence.translate(trans) \n",
    "            if sentence not in uniquesentences:\n",
    "                uniquesentences.add(sentence.lower())\n",
    "\n",
    "    print(f\"We now have {len(uniquesentences)} unique sentences.\")\n",
    "              \n",
    "    del(artcls)\n",
    "    \n",
    "    print('Saving uniquesentences.txt:')\n",
    "    with open('uniquesentences.txt', mode='w') as fo:\n",
    "      for sentence in tqdm(uniquesentences):\n",
    "        fo.write(sentence)\n",
    "\n",
    "    del(uniquesentences)\n",
    "    os.system('rm newspapers -r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k91FQsjGsLxd",
   "metadata": {
    "id": "k91FQsjGsLxd"
   },
   "outputs": [],
   "source": [
    "# loading unique sentences\n",
    "print('Loading unique sentences...')\n",
    "tokenizedsentences = []\n",
    "with open('drive/MyDrive/uniquesentences.txt', mode='r') as fi:\n",
    "  reader = csv.reader(fi)\n",
    "  for sentence in tqdm(reader):\n",
    "    tokenizedsentences.append(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spkoV479rBV9",
   "metadata": {
    "id": "spkoV479rBV9"
   },
   "outputs": [],
   "source": [
    "print(f\"Started setting up the model at {datetime.now()}\")\n",
    "model = gensim.models.Word2Vec(size=300, min_count=100) # we want 300 dimensions and not overdo it with the features\n",
    "model.build_vocab(tokenizedsentences)\n",
    "print(f\"Started training at {datetime.now()}\")\n",
    "model.train(tokenizedsentences, total_examples=model.corpus_count,  epochs=1)\n",
    "print(f\"Finished training at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J-2XB8dQu6Cl",
   "metadata": {
    "id": "J-2XB8dQu6Cl"
   },
   "outputs": [],
   "source": [
    "print('Saving model:')\n",
    "model.save(\"np_emb\")\n",
    "print('Model finished!')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "emb_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
